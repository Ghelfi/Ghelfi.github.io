---
layout: single
title: "Welcome under the Tensors"
permalink: /
author_profile: true
---

## Why ‚ÄúUnder the Tensors‚Äù?

Machine learning has reached incredible heights and keeps growing fast. A lot of focus is put on the *what* ‚Äî like in `"What is attention and why is it useful?"` ‚Äî but not so much on *how* it works from an engineering point of view.

**Under the Tensors** is where I explore those details: from PyTorch internals and GPU kernels to model compilation and optimisation techniques like FlashAttention.  
I want to share what I learn while digging deep into ML engineering, GPU programming, and performance optimisation ‚Äî not just *what* works, but *how* and *why*.  

---

## What You'll Find Here

- In-depth detailed explorations of PyTorch internals, CUDA kernels, and GPU-accelerated algorithms.
- Step-by-step optimisation decompositions breaking down how to profile, debug, and speed up ML pipelines.
- Walkthroughs of SOTA ML Engineering papers, with annotated code and experiments that make complex methods tangible.

The different pages are star-coded to provide a quick understanding of the technical depth:

- ‚≠ê One Star: Discussion accessible to any passionate reader ‚Äî does not require any technical skills.
- ‚≠ê‚≠ê Two Stars: The bulk of the discussion. We will be touching on how PyTorch works under the hood ‚Äî requires basic knowledge of PyTorch and Python.
- ‚≠ê‚≠ê‚≠ê Three Stars: Entering misty territory. We are touching low-level optimisation and discussing SOTA articles on ML engineering ‚Äî probably requires a bit of passion too.


<div style="background-color: #d4edda; border: 1px solid #c3e6cb; padding: 15px; border-radius: 5px; color: #000000ff; font-weight: bold; margin: 20px 0;">
If you‚Äôre ready, let's discover how it all works beneath the surface!<br>
Pick your article from the left menu bar! üëà
</div>

---
