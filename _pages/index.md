---
layout: single
title: "Welcome Under the Tensor"
permalink: /
author_profile: true
---

## Why “Under the Tensor”?

Machine learning has reached incredible heights and keeps growing fast. A lot of focus is put on the *what* — what attention is and why it is useful — but not so much on *how* it works from an engineering point of view.  
**Under the Tensor** is where I explore those details: from PyTorch internals and GPU kernels to model compilation and optimisation techniques like FlashAttention.  

I want to share what I learn while digging deep into ML engineering, GPU programming, and performance optimisation — not just *what* works, but *how* and *why*.  

---

## What You'll Find Here

- **Deep Dives** — Step-by-step explorations of PyTorch internals, CUDA kernels, and GPU-accelerated algorithms.  
- **Performance Insights** — Profiling, debugging, and optimising ML pipelines at scale.  
- **Implementation Walkthroughs** — Annotated code and experiments that make complex methods more tangible.  

The different pages are star-coded to provide a quick understanding of the technical depth:

- ⭐ One Star: Discussion accessible to any passionate reader — does not require any technical skills.
- ⭐⭐ Two Stars: The bulk of the discussion. We will be touching on how PyTorch works under the hood — requires basic knowledge of PyTorch and Python.
- ⭐⭐⭐ Three Stars: Entering misty territory. We are touching low-level optimisation and discussing SOTA articles on ML engineering — probably requires a bit of passion too.

---

## Sections (coming soon)

---
